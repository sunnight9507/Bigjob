{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n빅데이터 - 비정형(텍스트) - HOW?\\n자연어처리 이용,\\n이유1. 온갖 잡다한 종류의 텍스트 포함\\n이유2. for feature selection\\n[Preprocessing]\\n[Tokenizing]\\n1. split, splitlines => 문서-문단(문장의 나열)-단어(어절)의 조합-형태소 조합\\n   tokenize\\n2. sent_tokenize, word_tokenize, regex_tokenize, Tweettokenize\\n   => 구두점(Punk), .?!, \\'\\', \"\", ,\\n[Lematization]\\n3. 국민의 국민에게, ...\\n   (형태학적-Morphological) 형태소분석-형태소분리(Konlpy)\\n   (통사론적-POS) 품사(체언 위주)\\n   (구문론적-ParseTree, Gramer) 주어-목적어-보어-술어\\n[Language Model]\\n4. LM(Language Model) : 언어의 패턴(생성 확률) => ML(NNLM->W2V)\\n   Out of Voca - 없는 단어가 정말 많다.\\n   Ngram - 빈도만으로 확률 가능 / 없는 단어, 패턴 찾기\\n[Stemming]\\n5. Stem(어근/어간) 공통된 말을 찾는법\\n   WPM-BPE 발전\\n   \\n6. Zipf\\'s Law\\n   => 고빈도 단어(빈도의 대부분을 차지) => 중요하지 않음 => Stopwords 제거\\n7. Normalization\\n   대소문자, 약어, 신조어(사전에 없는 단어), 명사추정, 불용어(Stopwords)\\n   => 욕(비속어)사전, ****\\n8. RE(regular expression)\\n======> Feature\\nD = {t1, t2, ...} => Vector, T\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "빅데이터 - 비정형(텍스트) - HOW?\n",
    "자연어처리 이용,\n",
    "이유1. 온갖 잡다한 종류의 텍스트 포함\n",
    "이유2. for feature selection\n",
    "[Preprocessing]\n",
    "[Tokenizing]\n",
    "1. split, splitlines => 문서-문단(문장의 나열)-단어(어절)의 조합-형태소 조합\n",
    "   tokenize\n",
    "2. sent_tokenize, word_tokenize, regex_tokenize, Tweettokenize\n",
    "   => 구두점(Punk), .?!, '', \"\", ,\n",
    "[Lematization]\n",
    "3. 국민의 국민에게, ...\n",
    "   (형태학적-Morphological) 형태소분석-형태소분리(Konlpy)\n",
    "   (통사론적-POS) 품사(체언 위주)\n",
    "   (구문론적-ParseTree, Gramer) 주어-목적어-보어-술어\n",
    "[Language Model]\n",
    "4. LM(Language Model) : 언어의 패턴(생성 확률) => ML(NNLM->W2V)\n",
    "   Out of Voca - 없는 단어가 정말 많다.\n",
    "   Ngram - 빈도만으로 확률 가능 / 없는 단어, 패턴 찾기\n",
    "[Stemming]\n",
    "5. Stem(어근/어간) 공통된 말을 찾는법\n",
    "   WPM-BPE 발전\n",
    "   \n",
    "6. Zipf's Law\n",
    "   => 고빈도 단어(빈도의 대부분을 차지) => 중요하지 않음 => Stopwords 제거\n",
    "7. Normalization\n",
    "   대소문자, 약어, 신조어(사전에 없는 단어), 명사추정, 불용어(Stopwords)\n",
    "   => 욕(비속어)사전, ****\n",
    "8. RE(regular expression)\n",
    "======> Feature\n",
    "D = {t1, t2, ...} => Vector, T\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.corpus import kolaw\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = kolaw.open(kolaw.fileids()[0]).read()\n",
    "\n",
    "a = defaultdict(lambda : 0)\n",
    "\n",
    "for _ in corpus.split():\n",
    "    a[_] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "국민생활의 1\n",
      "국민투표에 3\n",
      "국민에게 2\n",
      "국민으로부터 1\n",
      "국민이 2\n",
      "국민전체에 1\n",
      "국민에 1\n",
      "국민의 7\n",
      "국민은 35\n",
      "국민을 1\n",
      "국민 2\n",
      "국민경제의 3\n",
      "국민경제자문회의를 1\n",
      "국민투표의 1\n",
      "국민투표사무에 1\n",
      "국민경제상 1\n"
     ]
    }
   ],
   "source": [
    "for k, v in a.items():\n",
    "    if k.startswith('국민'):\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = defaultdict(lambda:0)\n",
    "R = defaultdict(lambda:0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in corpus.split():\n",
    "    if len(_) > 1:\n",
    "        for i in range(1, len(_)-1):\n",
    "            L[_[:i+1]] += 1\n",
    "            R[_[i+1:]] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('법률', 123),\n",
       " ('국회', 77),\n",
       " ('의하', 72),\n",
       " ('대통', 68),\n",
       " ('대통령', 63),\n",
       " ('국민', 61),\n",
       " ('아니', 58),\n",
       " ('헌법', 57),\n",
       " ('있다', 57),\n",
       " ('한다', 56)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(L.items(), key=lambda _:_[1],\n",
    "       reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('의', 415),\n",
       " ('.', 339),\n",
       " ('는', 274),\n",
       " ('에', 230),\n",
       " ('을', 205),\n",
       " ('은', 184),\n",
       " ('다.', 172),\n",
       " ('이', 148),\n",
       " ('여', 145),\n",
       " ('조', 137)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(R.items(), key=lambda _:_[1],\n",
    "       reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Hannanum, Kkma, Okt, Komoran\n",
    "\n",
    "kkma = Kkma()\n",
    "hann = Hannanum()\n",
    "okt = Okt()\n",
    "kom = Komoran()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '팩폭의 수위가 느껴진다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('팩', 'NNG'),\n",
       "  ('폭', 'NNG'),\n",
       "  ('의', 'JKG'),\n",
       "  ('수위', 'NNG'),\n",
       "  ('가', 'JKS'),\n",
       "  ('느껴지', 'VV'),\n",
       "  ('ㄴ다', 'EFN'),\n",
       "  ('.', 'SF')],\n",
       " [('팩폭', 'Noun'),\n",
       "  ('의', 'Josa'),\n",
       "  ('수위', 'Noun'),\n",
       "  ('가', 'Josa'),\n",
       "  ('느껴진다', 'Verb'),\n",
       "  ('.', 'Punctuation')],\n",
       " [('팩폭', 'N'),\n",
       "  ('의', 'J'),\n",
       "  ('수위', 'N'),\n",
       "  ('가', 'J'),\n",
       "  ('느끼', 'P'),\n",
       "  ('어', 'E'),\n",
       "  ('지', 'P'),\n",
       "  ('ㄴ다', 'E'),\n",
       "  ('.', 'S')],\n",
       " [('팩', 'NNG'),\n",
       "  ('폭', 'NNG'),\n",
       "  ('의', 'JKG'),\n",
       "  ('수위', 'NNP'),\n",
       "  ('가', 'JKS'),\n",
       "  ('느끼', 'VV'),\n",
       "  ('어', 'EC'),\n",
       "  ('지', 'VX'),\n",
       "  ('ㄴ다', 'EF'),\n",
       "  ('.', 'SF')])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kkma.pos(s), okt.pos(s), hann.pos(s), kom.pos(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "팩폭의\n",
      "팩폭 0\n",
      "의 415\n",
      "\n",
      "수위가\n",
      "수위 0\n",
      "가 44\n",
      "\n",
      "느껴진다.\n",
      "느껴 0\n",
      "진다. 0\n",
      "느껴진 0\n",
      "다. 172\n",
      "느껴진다 0\n",
      ". 339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in s.split():\n",
    "    if len(_) > 1:\n",
    "        print(_)\n",
    "        for i in range(1, len(_)-1):\n",
    "            print(_[:i+1], L[_[:i+1]])\n",
    "            print(_[i+1:], R[_[i+1:]])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.corpus import kobill\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "corpus = ''.join([kobill.open(_).read() for _ in kobill.fileids()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "voca = word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = dict()\n",
    "R = dict()\n",
    "for term in voca:\n",
    "    if len(term) > 1:\n",
    "        for i in range(1, len(term)-1):\n",
    "            lkey = term[:i+1]\n",
    "            rkey = term[i+1:]\n",
    "            if lkey in L.keys():\n",
    "                L[lkey] += 1\n",
    "            else:\n",
    "                L[lkey] = 0\n",
    "                \n",
    "            if rkey in R.keys():\n",
    "                R[rkey] += 1\n",
    "            else:\n",
    "                R[rkey] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('육아', 147),\n",
       "  ('육아휴', 146),\n",
       "  ('육아휴직', 125),\n",
       "  ('20', 102),\n",
       "  ('제1', 86),\n",
       "  ('발생', 77),\n",
       "  ('201', 70),\n",
       "  ('행정', 69),\n",
       "  ('이하', 66),\n",
       "  ('육아휴직자', 55)],\n",
       " [('의', 380),\n",
       "  ('에', 319),\n",
       "  ('는', 253),\n",
       "  ('을', 243),\n",
       "  ('를', 163),\n",
       "  ('로', 150),\n",
       "  ('한', 138),\n",
       "  ('다', 136),\n",
       "  ('이', 122),\n",
       "  ('과', 121)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(L.items(), key=lambda _ : _[1], reverse=True)[:10], sorted(R.items(), key=lambda _ : _[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L + (R) : (R)\n",
    "# L + R : 가장 큰 R split\n",
    "# ==> L 명사범주 추정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'가수 에릭남이 철피엠에 출연해 입담을 뽐냈다'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"가수 에릭남이 '철피엠'에 출연해 입담을 뽐냈다.\"\n",
    "s = re.sub('[{}]'.format(re.escape(punctuation)), '', s)\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가수\n",
      "명사 추정\n",
      "\n",
      "에릭남이\n",
      "{'이': 1.0}\n",
      "에릭남\n",
      "\n",
      "철피엠에\n",
      "{'에': 1.0}\n",
      "철피엠\n",
      "\n",
      "출연해\n",
      "명사 추정\n",
      "\n",
      "입담을\n",
      "{'을': 1.0}\n",
      "입담\n",
      "\n",
      "뽐냈다\n",
      "{'다': 1.0}\n",
      "뽐냈\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for term in word_tokenize(s):\n",
    "    if len(term) > 1:\n",
    "        print(term)\n",
    "        prob = dict()\n",
    "        for i in range(1, len(term)-1):\n",
    "            rkey = term[i+1:]\n",
    "            if rkey in R.keys():\n",
    "                prob[rkey] = R[rkey]\n",
    "                \n",
    "        total = sum(prob.values())\n",
    "        if len(prob.keys()) == 0:\n",
    "            print('명사 추정')\n",
    "        else:\n",
    "            maxkey = max(prob, key=prob.get)\n",
    "            print({k:v/total for k, v in prob.items()})\n",
    "            print(re.sub(maxkey, '', term))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 불용어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 ** 짜증나\n"
     ]
    }
   ],
   "source": [
    "stopwords = ['씨발']\n",
    "s = '아 씨발 짜증나'\n",
    "_s = list()\n",
    "\n",
    "for _ in word_tokenize(s):\n",
    "    if _ in stopwords:\n",
    "        _s.append('*' * len(_))\n",
    "    else:\n",
    "        _s.append(_)\n",
    "        \n",
    "print(' '.join(_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 포장 선물포장 씨1발 짜증나\n",
      "아 포장 선물포장 씨1발 짜증나\n"
     ]
    }
   ],
   "source": [
    "stopwords = ['씨발', '시발']\n",
    "s = '아 포장 선물포장 씨1발 짜증나'\n",
    "s = re.sub('[{}]'.format(re.escape(punctuation)), '', s)\n",
    "\n",
    "print(s)\n",
    "\n",
    "_s = list()\n",
    "\n",
    "for _ in word_tokenize(s):\n",
    "    if re.search(r'({})'.format('|'.join(stopwords)), _):\n",
    "        _s.append('*' * len(_))\n",
    "    else:\n",
    "        _s.append(_)\n",
    "        \n",
    "print(' '.join(_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE\n",
    "import re\n",
    "\n",
    "def convert_data(data): # {'문자열' : 빈도}\n",
    "    newdata = dict()\n",
    "    for k,v in data.items():\n",
    "        newdata['</w>' + ' '.join(list(k)) + '</w>'] = v\n",
    "    return newdata\n",
    "\n",
    "def find_pair(data): # 문자열 => 쌍으로 변환\n",
    "    pair = defaultdict(lambda : 0)\n",
    "    for k, v in data.items(): # l o w </w> : 5\n",
    "        tokens = k.split() # [l, o, w, </w>]\n",
    "        for i in range(len(tokens) - 1): # (l, 0) : 5, (0, w) : 5\n",
    "            pair[tuple(tokens[i:i+2])] += v\n",
    "    return pair\n",
    "\n",
    "def merge_pair(data, key): # Max pattern => 합치기\n",
    "    newdata = dict()\n",
    "    for k, v in data.items():\n",
    "        newkey = re.sub(' '.join(key), ''.join(key), k) # 'e s'\n",
    "        newdata[newkey] = v\n",
    "    return newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = convert_data({'씨발' : 5, '시발' : 2, '씨-발' : 6, '씨!@#$발' : 3})\n",
    "\n",
    "for _ in range(3):\n",
    "    pair = find_pair(data)\n",
    "    key = max(pair, key=pair.get) # 'e s'\n",
    "    data = merge_pair(data, key)\n",
    "    \n",
    "temp = list(set([token for _ in data.keys() \n",
    "                 for token in _.split()\n",
    "                 if len(token) > 1 and token != '</w>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _s = list()\n",
    "# for token in s.split():\n",
    "#     skip = False\n",
    "#     for p in pattern:\n",
    "#         if p.search('<w>' + token + '<w>'):\n",
    "#             skip = True\n",
    "#     if skip:\n",
    "#         _s.append('*' * len(token))\n",
    "#     else:\n",
    "#         _s.append(token)\n",
    "# print(' '.join(_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IR Componets\n",
    "\n",
    "- 1) Crawler + indexer -> Crawler / Indexer\n",
    "- 2) Doc Analyzer -> (Improved) Bow\n",
    "- 3) Query -> 2번과 동일 -> DTM(TDM)\n",
    "- 4) Ranking Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests.compat import urljoin\n",
    "from os import listdir\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from string import punctuation\n",
    "from konlpy.tag import Okt\n",
    "from collections import defaultdict\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "pattern1 = re.compile(r'[{}]'.format(re.escape(punctuation)))\n",
    "pattern2 = re.compile(r'\\b(\\w|[.])+@(?:[.]?\\w+)+\\b')\n",
    "pattern3 = re.compile(r'\\bhttps?://\\w+(?:[.]?\\w+)+\\b')\n",
    "pattern4 = re.compile(r'[^A-Za-z가-힣0-9ㄱ-ㅎㅏ-ㅣ ]')\n",
    "pattern5 = re.compile(r'\\b[a-z][A-Za-z0-9]+\\b')\n",
    "pattern6 = re.compile(r'\\s{2,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "정치\n",
      "“조국은 파렴치한” 비장해진 국대떡볶이 대표 페북 상황\n",
      "0001348190\n",
      "[단독] 월세 몸소 실천한다던 윤준병, 알고보니 정읍 지역구\n",
      "0003551166\n",
      "\"오거돈·박원순 성범죄 맞나\" 묻자… 끝까지 입닫은 여가부 장관\n",
      "0003551227\n",
      "윤준병 “정읍에서 월세 산다…큰 금액 내고 있지는 않아”\n",
      "0001348185\n",
      "[속보] ‘월세예찬’ 논란 윤준병, 선거법 위반 혐의 기소\n",
      "0003490198\n",
      "\"오거돈·박원순, 권력형 성범죄 맞냐\" 물음에 끝까지 답 못한 여가부 장관\n",
      "0000521888\n",
      "잠자던 '야성' 깨운 윤희숙…통합당 \"합리적 '대안 야당' 되자\"\n",
      "0004391547\n",
      "\"부동산 폭등은 MB·박근혜 정부 때문\"…오늘도 '남탓'한 與\n",
      "0004391479\n",
      "[레이더P] 바뀐 서울 민심…정당지지율 서울서 통합당이 민주당 앞서\n",
      "0004627658\n",
      "[1보] 외교부 \"성추행 의혹 외교관 오늘부로 즉각 귀임 발령\"\n",
      "0011788427\n",
      "101\n",
      "경제\n",
      "미국인이 42채 '싹쓸이' 中유학생은 8채 사서 임대... 만만했던 한국 부동산\n",
      "0000521902\n",
      "갭투자로 아파트 42채 쓸어 담은 미국인, 세무조사 착수\n",
      "0003551211\n",
      "일회용마스크 4종 피부염 유발 우려… 리콜 권고\n",
      "0003024464\n",
      "2년만에 아파트 42채 갭투자 미국인…외국인 다주택자 세무조사\n",
      "0011787910\n",
      "미국인이 갭투자로 42채 구입…외국인도 아파트 쓸어담았다\n",
      "0003022999\n",
      "미국 \"일본의 수출규제는 WTO 심리 대상 아니다\"…일본 편들어\n",
      "0011787924\n",
      "[단독] 文 정부 3년 집값 급등 상위 지역 보니…충청권 '싹쓸이'\n",
      "0004391534\n",
      "부모가 집 있으면 미혼 20대에 취득세 폭탄?… \"중위소득 40% 이상은 독립가구 인정\"\n",
      "0004729825\n",
      "김현아 “임대차법 최대 수혜자, 강남 10억 고액 전세자”\n",
      "0003112561\n",
      "‘갭투자’로 42채…외국인 집주인들 세무조사 착수\n",
      "0010879130\n",
      "102\n",
      "사회\n",
      "천안 ‘집중호우경보’ 발령…‘침수 우려’ 성정지하차도 통제\n",
      "0003301200\n",
      "친구 살해 뒤 무의도에 버렸다…'가방 시신' 사건 20대들 자수\n",
      "0003023007\n",
      "[속보] 강원 홍천군 캠핑장 확진자, 강남 커피숍 머물러...방역당국 \"관련성 확인 중\"\n",
      "0000521914\n",
      "[속보] 가평 펜션에 토사 밀려 들어와… 어린이 등 3명 실종\n",
      "0000521905\n",
      "\"승객 방귀에 이성 잃었다\" 흉기 10여차례 휘두른 택시기사\n",
      "0003022980\n",
      "가평서 토사에 펜션 매몰돼 구조작업중…\"3명 대피 못 해\"\n",
      "0011788202\n",
      "[속보] 서울 집중호우로 올림픽대로 염창IC~동작대교 교통통제\n",
      "0003024492\n",
      "친구 살해 후 인천 무의도에 시신 유기 20대 2명 체포\n",
      "0000521892\n",
      "평택 공장에 토사 덮쳐 3명 사망·1명 중상(종합)\n",
      "0011788086\n",
      "[단독] '한 방' 못찾은 채널A 수사팀, 기소 전날 또 노트북 포렌식\n",
      "0003551217\n",
      "103\n",
      "생활문화\n",
      "'IQ 164' 3살 영재의 놀라운 성장…비결은 부모다\n",
      "0000832053\n",
      "\"오히려 기분 나빠\"…영재 망치는 '독'되는 칭찬\n",
      "0000832054\n",
      "`장대비 내려도 나는 달린다`…위험천만 러닝族 \"뿌듯하다\"\n",
      "0004627628\n",
      "진로 바뀐 4호 태풍 '하구핏'...우리나라로 올 가능성은?\n",
      "0001472759\n",
      "“병 생긴 뒤 후회한다” 당장 토마토가 필요한 사람들 5\n",
      "0000045890\n",
      "폭우로 침수된 KTX천안아산역 인근 지하도\n",
      "0011788124\n",
      "[뉴스피처] \"남자 만날 수 있다\" 영상 올렸다가…감옥 가게 된 여대생\n",
      "0011786953\n",
      "[더오래]모기가 얼마나 싫었으면…정약용이 남긴 시 ‘증문’\n",
      "0003023010\n",
      "'돌밥돌밥' 이제 못하겠어요... 밥 프리를 선언합니다\n",
      "0002279185\n",
      "그때 그 감성 그대로…'추억 소환' 뉴트로 게임 열풍｜아침& 라이프\n",
      "0000244183\n",
      "104\n",
      "세계\n",
      "34살 세계 최연소 핀란드 총리, 코로나 꺾이자 드디어 결혼\n",
      "0003551212\n",
      "틱톡 퇴출 이갈더니 MS 인수 사실상 승인…트럼프 돌변, 왜\n",
      "0003023003\n",
      "美 곳곳으로 배달된 중국발 '의문의 씨앗' 정체 밝혀져\n",
      "0001472887\n",
      "태풍 하구핏, 중국 향한다…샨샤댐 수위 조절에 초긴장\n",
      "0000890782\n",
      "“피라미드 외계인이 만들었다”는 머스크에게 이집트가 통 큰 초청장을 날렸다\n",
      "0002507337\n",
      "교도관 쓰러지자 60명이 일제히 철문 두드려…美 수감자들 화제\n",
      "0003490185\n",
      "英 해변서 4.5m 괴생명체 사체 발견…“고래로 추정”\n",
      "0003112589\n",
      "한국인은 돈 내고 물 마셔라?…日 유명 초밥집의 혐한\n",
      "0003390684\n",
      "212년 된 조각상 발가락이 똑, 셀카 찍고 도망간 남성\n",
      "0003551206\n",
      "美, WTO 회의장서 \"수출규제는 日 안보조치\" 발언 파문\n",
      "0003023022\n",
      "105\n",
      "IT과학\n",
      "“갤노트20 대신 싼 노트10 사자!”…‘반짝’ 특수 [IT선빵!]\n",
      "0001707231\n",
      "27만명→3만명… 네이버통장 ‘한달천하’ [IT선빵!]\n",
      "0001707186\n",
      "'괴짜 천재' 일론 머스크가 막 올린 민간 우주여행 시대\n",
      "0004449443\n",
      "‘배터리 전쟁’ 韓 잰걸음 vs 日·中 뒷걸음…승자는 LG화학\n",
      "0002089610\n",
      "“갤노트20 대신 싼 갤노트10 사자!”…반짝 특수 [IT선빵!]\n",
      "0001707363\n",
      "美 첫 민간 유인우주선 지구 귀환, 우주에서 봤더니…\n",
      "0002195462\n",
      "일론 머스크도 감탄…미국 우주인, 45년 만에 해상 귀환\n",
      "0000890611\n",
      "“누가 콜록했는지 다 압니다”…‘기침 인식 카메라’ 국내 개발\n",
      "0002507356\n",
      "보령제약, 소세포폐암 신약 ‘러비넥테딘’ 희귀의약품 지정\n",
      "0000564412\n",
      "“반갑다 중력아!”…美 첫 유인우주선 ‘크루드래건’ 귀환 성공[청계천 옆 사진관]\n",
      "0003301199\n"
     ]
    }
   ],
   "source": [
    "for news in dom.select('div[id^=ranking_]'):\n",
    "    print(re.search('\\d{3}', news['id']).group())\n",
    "    print(pattern1.sub('', news.find().text))\n",
    "    \n",
    "    for a in news.select('li > a'):\n",
    "        link = urljoin(seed, a['href'])\n",
    "#         print(urljoin(seed, a['href']))\n",
    "        print(a.text.strip())\n",
    "        print(re.search('aid=(\\d+)', link).group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-1. (Focused) Crawler (BFS) - 네이버 뉴스\n",
    "urls = ['https://news.naver.com']\n",
    "visited = list()\n",
    "\n",
    "path = './naver/'\n",
    "\n",
    "while urls:\n",
    "    seed = urls.pop(0) # Queue\n",
    "    visited.append(seed)\n",
    "    \n",
    "    dom = BeautifulSoup(get(seed).text, 'html.parser')\n",
    "    body = dom.select_one('#articleBodyContents')\n",
    "    \n",
    "    if body: # 뉴스\n",
    "        cid = re.search('rankingSectionId=(\\d+)', seed).group(1)\n",
    "        aid = re.search('aid=(\\d+)', seed).group(1)\n",
    "        file = '{0}-{1}.txt'.format(cid, aid)\n",
    "        with open(path+file, 'w', encoding='utf-8') as f:\n",
    "            f.write(body.text)\n",
    "            \n",
    "    else:# 링크\n",
    "        for a in dom.select('div[id^=ranking_] li > a'):\n",
    "            link = urljoin(seed, a['href'])\n",
    "            if link not in urls and link not in visited:\n",
    "                urls.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-2. Indexer\n",
    "def fileids(path = './naver/'):\n",
    "    return [path + _ for _ in listdir(path) if re.search('[.]txt$', _)]\n",
    "\n",
    "def cleaning(doc):\n",
    "    return pattern6.sub(' ', \n",
    "            pattern1.sub(' ', \n",
    "            pattern5.sub(' ', \n",
    "            pattern4.sub(' ', \n",
    "            pattern3.sub(' ',\n",
    "            pattern2.sub(' ', doc)))))).strip()\n",
    "\n",
    "def tokenizer1(doc): # 어절\n",
    "    return doc.split()\n",
    "\n",
    "def tokenizer2(tokens, n=2): # 어절 Ngarm\n",
    "    ngram = list()\n",
    "    for i in range(len(tokens) - (n-1)):\n",
    "        ngram.append(tokens[i:i+n])\n",
    "    return ngram\n",
    "\n",
    "def tokenizer3(tokens, n=2): # 음절 Ngram\n",
    "    ngram = list()\n",
    "    for i in range(len(doc) - (n-1)):\n",
    "        ngram.append(tokens[i:i+n])\n",
    "    return ngram\n",
    "\n",
    "def tokenizer4(doc): # 형태소\n",
    "    return [_ for _ in okt.morphs(doc) if 1 < len(_) < 10]\n",
    "\n",
    "def tokenizer5(doc): # 명사\n",
    "    return [_ for _ in okt.nouns(doc) if 1 < len(_) < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fileids()[0], 'r',encoding='utf8') as f:\n",
    "    news = cleaning(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257 358\n",
      "823 1745\n",
      "823 3132\n",
      "837 3485\n",
      "837 3743\n"
     ]
    }
   ],
   "source": [
    "tokens = defaultdict(lambda:0)\n",
    "for _ in tokenizer1(news):\n",
    "    tokens[_] += 1\n",
    "print(len(tokens), sum(tokens.values()))\n",
    "\n",
    "for _ in tokenizer2(news):\n",
    "    tokens[_] += 1\n",
    "print(len(tokens), sum(tokens.values()))\n",
    "\n",
    "for _ in tokenizer3(news):\n",
    "    tokens[_] += 1\n",
    "print(len(tokens), sum(tokens.values()))\n",
    "\n",
    "for _ in tokenizer4(news):\n",
    "    tokens[_] += 1\n",
    "print(len(tokens), sum(tokens.values()))\n",
    "\n",
    "for _ in tokenizer5(news):\n",
    "    tokens[_] += 1\n",
    "print(len(tokens), sum(tokens.values()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
